{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267eec8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "此示例展示了如何使用 vLLM 运行 Qwen/Qwen2.5-VL-3B-Instruct 模型进行离线推断，\n",
    "并使用正确的提示格式进行文本生成。\n",
    "\"\"\"\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import asdict\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "# from transformers import AutoTokenizer # Qwen2.5-VL 的 run_qwen2_5_vl 中不直接使用\n",
    "\n",
    "from vllm import LLM, EngineArgs, SamplingParams\n",
    "from vllm.assets.image import ImageAsset\n",
    "# from vllm.assets.video import VideoAsset # 如果要测试视频，取消注释\n",
    "from vllm.lora.request import LoRARequest # ModelRequestData 中需要，但此模型默认为 None\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "\n",
    "\n",
    "class ModelRequestData(NamedTuple):\n",
    "    engine_args: EngineArgs\n",
    "    prompts: list[str]\n",
    "    stop_token_ids: Optional[list[int]] = None\n",
    "    lora_requests: Optional[list[LoRARequest]] = None\n",
    "\n",
    "\n",
    "# Qwen2.5-VL\n",
    "def run_qwen2_5_vl(questions: list[str], modality: str) -> ModelRequestData:\n",
    "\n",
    "    model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=4096,\n",
    "        max_num_seqs=5,  # 根据你的 GPU 调整\n",
    "        mm_processor_kwargs={\n",
    "            \"min_pixels\": 28 * 28,\n",
    "            \"max_pixels\": 1280 * 28 * 28,\n",
    "            \"fps\": 1, # 对于图像，fps 通常不直接使用，但保持与原脚本一致\n",
    "        },\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "        # trust_remote_code=True, # Qwen 模型通常需要，vLLM 会自动处理\n",
    "    )\n",
    "\n",
    "    if modality == \"image\":\n",
    "        placeholder = \"<|image_pad|>\"\n",
    "    elif modality == \"video\":\n",
    "        placeholder = \"<|video_pad|>\"\n",
    "    else:\n",
    "        raise ValueError(f\"Qwen2.5-VL 不支持的模态: {modality}\")\n",
    "\n",
    "\n",
    "    prompts = [\n",
    "        (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "         f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n",
    "         f\"{question}<|im_end|>\\n\"\n",
    "         \"<|im_start|>assistant\\n\") for question in questions\n",
    "    ]\n",
    "    \n",
    "    # Qwen2.5-VL 通常使用其 tokenizer.eos_token_id 作为停止符，\n",
    "    # 但 vLLM 的 SamplingParams 也可以通过 stop=['<|im_end|>'] 等字符串设置\n",
    "    # 这里我们不显式设置 stop_token_ids，让模型自然停止或依赖 max_tokens\n",
    "    # 如果需要特定停止符，可以像原始脚本中其他模型那样加载 tokenizer 并获取 ids\n",
    "    stop_token_ids = None # 例如： [tokenizer.eos_token_id] 或特定 ids\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "        stop_token_ids=stop_token_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_multi_modal_input(modality: str, num_frames: int = 16):\n",
    "    \"\"\"\n",
    "    返回:\n",
    "    {\n",
    "        \"data\": image or video,\n",
    "        \"questions\": question list,\n",
    "    }\n",
    "    \"\"\"\n",
    "    if modality == \"image\":\n",
    "        # 输入图像和问题\n",
    "        image = ImageAsset(\"cherry_blossom\") \\\n",
    "            .pil_image.convert(\"RGB\")\n",
    "        img_questions = [\n",
    "            \"What is the content of this image?\",\n",
    "            \"Describe the content of this image in detail.\",\n",
    "            \"这张图片里有什么？请用中文回答。\",\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"data\": image,\n",
    "            \"questions\": img_questions,\n",
    "        }\n",
    "\n",
    "    # if modality == \"video\": # 如果要测试视频，取消注释并提供视频资源\n",
    "    #     # 输入视频和问题\n",
    "    #     video = VideoAsset(name=\"baby_reading\",\n",
    "    #                        num_frames=num_frames).np_ndarrays\n",
    "    #     vid_questions = [\"Why is this video funny?\"]\n",
    "    #\n",
    "    #     return {\n",
    "    #         \"data\": video,\n",
    "    #         \"questions\": vid_questions,\n",
    "    #     }\n",
    "\n",
    "    msg = f\"当前测试脚本不支持的模态 {modality} 或未实现。\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def time_counter(enable: bool):\n",
    "    if enable:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        yield\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"-- generate time = {elapsed_time:.2f} 秒\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "\n",
    "def main_test_qwen2_5_vl():\n",
    "    # ---- 可配置参数 ----\n",
    "    target_model_func = run_qwen2_5_vl\n",
    "    current_modality = \"image\" # \"image\" 或 \"video\"\n",
    "    num_prompts_to_run = 2     # 你想运行多少个提示（会从 get_multi_modal_input 的 questions 中选取）\n",
    "    vllm_seed = 42             # vLLM 引擎的随机种子\n",
    "    time_generation = True     # 是否打印生成时间\n",
    "    # ---------------------\n",
    "\n",
    "    print(f\"开始测试模型: Qwen/Qwen2.5-VL-3B-Instruct, 模态: {current_modality}\")\n",
    "\n",
    "    mm_input = get_multi_modal_input(modality=current_modality)\n",
    "    data = mm_input[\"data\"]\n",
    "    # 选择部分或全部问题\n",
    "    questions = mm_input[\"questions\"][:num_prompts_to_run] \n",
    "    if not questions:\n",
    "        print(\"没有提供问题，测试中止。\")\n",
    "        return\n",
    "\n",
    "    req_data = target_model_func(questions, current_modality)\n",
    "\n",
    "    # 禁用其他模态以节省内存 (从原始脚本借鉴)\n",
    "    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n",
    "    engine_args_dict = asdict(req_data.engine_args)\n",
    "    engine_args_dict[\"limit_mm_per_prompt\"] = default_limits | dict(\n",
    "        req_data.engine_args.limit_mm_per_prompt or {}\n",
    "    )\n",
    "    \n",
    "    # 合并其他参数\n",
    "    engine_args_final = engine_args_dict | {\n",
    "        \"seed\": vllm_seed,\n",
    "        \"disable_mm_preprocessor_cache\": False, # 你可以设为 True 来测试禁用缓存的效果\n",
    "        \"trust_remote_code\": True, # 对于很多 HuggingFace 模型是必需的\n",
    "    }\n",
    "    \n",
    "    print(f\"初始化 LLM 引擎，参数: {engine_args_final}\")\n",
    "    llm = LLM(**engine_args_final)\n",
    "\n",
    "    # 我们设置 temperature 为 0.2 以便在批量推断时即使提示相同也能得到略微不同的输出。\n",
    "    # 对于 Qwen2.5-VL，你可能也想通过 stop 参数指定 '<|im_end|>' 等\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        max_tokens=256, # 根据需要调整\n",
    "        stop_token_ids=req_data.stop_token_ids,\n",
    "        # stop=[\"<|im_end|>\"] # 也可以用字符串形式的停止符\n",
    "    )\n",
    "    \n",
    "    print(f\"采样参数: {sampling_params}\")\n",
    "\n",
    "    inputs_for_llm = []\n",
    "    for i in range(len(questions)):\n",
    "        inputs_for_llm.append({\n",
    "            \"prompt\": req_data.prompts[i],\n",
    "            \"multi_modal_data\": {\n",
    "                current_modality: data\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # LoRA 请求（如果适用）\n",
    "    # Qwen2.5-VL 的 req_data.lora_requests 默认为 None\n",
    "    lora_request_list = (req_data.lora_requests * len(inputs_for_llm)\n",
    "                         if req_data.lora_requests else None)\n",
    "\n",
    "    print(f\"\\n开始生成 {len(inputs_for_llm)} 个回复...\")\n",
    "    with time_counter(time_generation):\n",
    "        outputs = llm.generate(\n",
    "            inputs_for_llm,\n",
    "            sampling_params=sampling_params,\n",
    "            lora_request=lora_request_list,\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"生成结果:\")\n",
    "    for i, o in enumerate(outputs):\n",
    "        original_prompt_question = questions[i]\n",
    "        # 构建一个简化的输入提示用于显示\n",
    "        display_prompt = req_data.prompts[i].split(\"<|im_start|>assistant\")[0] + \"<|im_start|>assistant\\n\"\n",
    "        \n",
    "        print(f\"\\n--- 问题 {i+1} ---\")\n",
    "        print(f\"问题文本: {original_prompt_question}\")\n",
    "        # print(f\"完整提示 (部分): {display_prompt}\") # 完整提示可能很长\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(f\"模型回复: {generated_text.strip()}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_test_qwen2_5_vl()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
