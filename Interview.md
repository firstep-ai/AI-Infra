# Tell me about yourself.

Hello! My name is Wang Zhipeng, and I hold a Master's degree in Intelligent Systems from NUS. I am a software engineer with strong experience in LLMs, particularly in inference optimization and model acceleration. I was a key contributor to the vLLM project, which is the most widely used inference engine globally, used by major tech companies like Google, Meta, and Amazon. My significant contributions included leading the integration of ByteDance's Tarsier multi-modal model into vLLM, which involved adding support for Tensor Parallelism, Pipeline Parallelism, and quantized models. Beyond inference, I am proficient in modern training methodologies and distributed computing. I have successfully applied these skills to build practical AI solutions for various companies, as demonstrated by my work at Yonyou. I am eager to bring my AI Infrastructure expertise to Rakuten.

# Why did you choose this industry/position?

I chose AI Infra and this job because I am very interested in building and making AI systems run better. My work on vLLM  showed me how important fast inference engines are for LLMs. I like solving hard engineering problems to make AI models work well and stable. Rakuten is a leader in technology and invests a lot in AI, which attracts me. I believe this job will let me grow more in AI Infra and help Rakuten's AI goals.

# What are your strengths and weaknesses?

My Strengths
My biggest strength is that I'm an AI Infra tool developer, not just a user. This means I can deeply optimize AI systems from their core code, unlike others who mostly just use tools like vLLM (which I helped build!). Also, my long time in open source means I know and can directly contact many of the world's top AI scientists and engineers. If Rakuten faces a big tech problem, I can use wechat directly ask the DeepSeek AI Infra Leader because I have his personal contact.

My Weaknesses
Sometimes, I get too focused on small technical details. I'm fixing this by joining more project planning and talking more with other teams, to make sure my work always helps the bigger business goals.

# What is your expected salary?

Based on my experience in AI Infra, especially with LLM inference and optimization, and my knowledge of salaries in Singapore, my expected monthly salary is 7000 SGD. I am also open to discuss this further based on your company's pay structure and benefits.

# What were your responsibilities in your previous role?

 I was a key person in the vLLM project. I led the full integration of ByteDance's Tarsier and Tarsier2 multi-modal model, including support for Tensor Parallelism and Pipeline Parallelism. I also fixed important issues to make vLLM more stable, like supporting long contexts for Mistral models and image/video tasks for Qwen2-VL models.

 In my current company, I led the full process from fine-tuning models to deploying them with vLLM. I also made a new Cache-Augmented Generation (CAG)-based KV-cache system that made inference much faster and more efficient. I also build knowledge bases and core parts for an AI assistant to make it accurate and efficient.

# What achievements have you made in your previous role?

My main achievements include:
For the Yonyou Translation Tool, my models performed better than GPT-4o in tests. My CAG-based KV-cache system made inference much faster and more efficient.
As a key person in the vLLM project, I successfully led the integration of ByteDance's Tarsier multi-modal model. This helped the open-source community use this complex model well. I also made vLLM more stable by fixing important problems, like handling long inputs for Mistral models.
In the "AI Sabah" project, I trained long-context models on only two GPUs, even with a small budget, by using a mix of parallel methods (PP + DP + ZeRO-3). Our model was 10% better than GPT-4o for function calls , and we won an international award.

# What is the biggest challenge you have faced? How did you overcome it?

My biggest challenge was adapting the Bagel model for vLLM. Bagel is the first open-source multi-modal to multi-modal CoT LLM, so optimizing its unique architecture was complex.
I started by posting in the AI community for ideas on image generation model inference. Soon, a research team from Stanford AI Lab reached out. They had a new algorithm, and we exchanged emails until I fully understood how to implement it.
A major hurdle was then adjusting vLLM's core architecture. This part is usually handled by Dr. Roger Wang, an OpenAI Principal Engineer, but he's very busy and we have a time difference. To overcome this, I sent him my schedule, and we successfully set up a video call to discuss the necessary architectural changes. This direct collaboration was key to moving forward.

# How do you feel about overtime?

I am positive and flexible about working overtime. I understand that in fast-moving areas like AI Infra, sometimes extra hours are needed to make sure systems are stable and projects are done. I am willing to work extra when needed to finish projects well. I believe that good work and results are most important, and I will be responsible for my tasks.

# What are your career goals for the next 3-5 years?

For the next 3-5 years, my goal is to deepen my expertise in AI Infrastructure, specifically optimizing LLM and multi-modal AI systems. I aim to become a leading expert capable of designing and spearheading complex AI architectures.

I also aspire to become a vLLM maintainer. Controlling the world's leading AI Infra framework would be incredibly impactful and aligns with my drive for low-level optimization.

I hope to join Rakuten's core AI Infra team, focusing on accelerating large LLM inference or pioneering new AI hardware optimizations. Ultimately, I want my contributions to directly fuel Rakuten's AI product growth and rapid deployment, delivering significant technical and business value.

# What do you hope to gain from this job?

From this job, I want to keep learning and growing, especially in large-scale AI Infra. I hope to work with Rakuten's top engineers to solve real problems in LLM deployment and optimization. I also want to use my vLLM and distributed computing experience in Rakuten's business. I want to work in a place that likes new ideas and technology, where my work can really help the company's AI plans.

# What do you know about our company?

I know a lot about Rakuten. Rakuten is a big global tech company with many businesses, like e-commerce, fintech, and digital content. I know that Rakuten invests a lot in AI and big data to make user experience better and business more efficient. Your company's vision for new technology and global growth is very interesting to me. I believe Rakuten's focus on AI Infra will be key for its future growth.

# Why do you think you are a good fit for this position?

I think I am a very good fit for Rakuten's AI Infra Engineer job for several reasons:
First, I have direct and deep experience in making LLM inference faster and building high-performance AI Infra. My work on vLLM shows I can optimize and integrate complex AI models for quick deployment.

Second, I have strong knowledge and practice in distributed computing (TP, PP, DP, ZeRO). This is very important for building and keeping big AI infrastructure. My experience in the "AI Sabah" project, where I fixed hardware limits, also shows I can solve problems.

# Do you have any questions for me?

Yes, I have some questions:
What kind of training and career path does Rakuten offer for new AI Infra engineers?
How would you describe the daily teamwork and new ideas culture in Rakuten's AI Infra team?

# What kind of team atmosphere do you prefer?

I want to work in a team that is positive, open, tech-focused, and supportive. I like it when team members can share technical ideas freely and solve hard AI Infra problems together. I also value a team that likes new ideas and learning, and where everyone helps each other grow. My experience working on the open-source vLLM project made me enjoy this kind of team very much.
